#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªØ³Øª Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ù…Ø´Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± Ø¹Ù…ÙˆÙ…ÛŒ
"""

import asyncio
import logging
import aiohttp
import xml.etree.ElementTree as ET
import html
import re
from datetime import datetime

# ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯ Ø¨Ø±Ø§ÛŒ debug
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/workspace/news_debug.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class DetailedNewsFetcher:
    def __init__(self):
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def test_single_rss_feed(self, source_name, url, limit=2, timeout=15):
        """ØªØ³Øª ÛŒÚ© Ù…Ù†Ø¨Ø¹ RSS Ø¨Ù‡ ØªÙ†Ù‡Ø§ÛŒÛŒ"""
        logger.info(f"ğŸ” ØªØ³Øª Ù…Ù†Ø¨Ø¹: {source_name}")
        logger.info(f"ğŸ”— URL: {url}")
        
        try:
            async with self.session.get(url, timeout=timeout) as response:
                logger.info(f"ğŸ“¡ ÙˆØ¶Ø¹ÛŒØª Ù¾Ø§Ø³Ø®: {response.status}")
                
                if response.status == 200:
                    content = await response.text()
                    logger.info(f"ğŸ“ Ø·ÙˆÙ„ Ù…Ø­ØªÙˆØ§: {len(content)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
                    
                    # Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† XML
                    try:
                        root = ET.fromstring(content)
                        items = root.findall('.//item')
                        logger.info(f"ğŸ“° ØªØ¹Ø¯Ø§Ø¯ item Ù‡Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡: {len(items)}")
                        
                        news_items = []
                        for i, item in enumerate(items[:limit]):
                            try:
                                title_elem = item.find('title')
                                link_elem = item.find('link')
                                description_elem = item.find('description')
                                pub_date_elem = item.find('pubDate')
                                
                                if title_elem is not None and link_elem is not None:
                                    title = html.unescape(title_elem.text or '').strip()
                                    link = link_elem.text or ''
                                    
                                    # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ ØªÙˆØ¶ÛŒØ­Ø§Øª
                                    description = ''
                                    if description_elem is not None and description_elem.text:
                                        desc_text = html.unescape(description_elem.text)
                                        desc_text = re.sub(r'<[^>]+>', '', desc_text)
                                        description = desc_text.strip()[:120] + '...' if len(desc_text) > 120 else desc_text.strip()
                                    
                                    # ØªØ§Ø±ÛŒØ® Ø§Ù†ØªØ´Ø§Ø±
                                    published = pub_date_elem.text if pub_date_elem is not None else ''
                                    
                                    news_items.append({
                                        'title': title,
                                        'link': link,
                                        'description': description,
                                        'source': source_name,
                                        'published': published
                                    })
                                    
                                    logger.info(f"  ğŸ“ Ø®Ø¨Ø± {i+1}: {title[:50]}...")
                                
                            except Exception as e:
                                logger.error(f"  âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ item {i+1}: {e}")
                        
                        logger.info(f"âœ… Ù…Ù†Ø¨Ø¹ {source_name}: {len(news_items)} Ø®Ø¨Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
                        return news_items
                        
                    except ET.ParseError as e:
                        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø§Ø±Ø³ XML: {e}")
                        return []
                        
                else:
                    logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø®: HTTP {response.status}")
                    return []
                    
        except asyncio.TimeoutError:
            logger.error(f"â° timeout Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² {source_name}")
            return []
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± {source_name}: {e}")
            return []
    
    async def test_all_news_sources(self):
        """ØªØ³Øª ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ"""
        news_sources = [
            # Ù…Ù†Ø§Ø¨Ø¹ Ø¯Ø§Ø®Ù„ÛŒ (ÙØ§Ø±Ø³ÛŒ) - Ù…Ù†Ø§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ limit Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡
            {
                'name': 'Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ù…Ù‡Ø±', 
                'url': 'https://www.mehrnews.com/rss',
                'limit': 3,
                'language': 'fa'
            },
            {
                'name': 'ØªØ³Ù†ÛŒÙ…',
                'url': 'https://www.tasnimnews.com/fa/rss/feed/0/8/0/%D8%A2%D8%AE%D8%B1%DB%8C%D9%86-%D8%A7%D8%AE%D8%A8%D8%A7%D8%B1',
                'limit': 3,
                'language': 'fa'
            },
            {
                'name': 'Ù…Ù‡Ø± Ø¹Ù…ÙˆÙ…ÛŒ',  # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† ÙØ§Ø±Ø³ Ø®Ø±Ø§Ø¨
                'url': 'https://www.mehrnews.com/rss/tp/1',
                'limit': 3,
                'language': 'fa'
            },
            {
                'name': 'Ù…Ù‡Ø± - Ø³ÛŒØ§Ø³ÛŒ',
                'url': 'https://www.mehrnews.com/rss/tp/sch',
                'limit': 2,
                'language': 'fa'
            },
            {
                'name': 'ØªØ³Ù†ÛŒÙ… - Ø¨ÛŒÙ†â€ŒØ§Ù„Ù…Ù„Ù„',
                'url': 'https://www.tasnimnews.com/fa/rss/feed/0/9/0/%D8%A8%DB%8C%D9%86-%D8%A7%D9%84%D9%85%D9%84%D9%84%D9%84',
                'limit': 2,
                'language': 'fa'
            },
            # Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø§Ø±Ø¬ÛŒ (Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ - Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ±Ø¬Ù…Ù‡)
            {
                'name': 'BBC World',
                'url': 'https://feeds.bbci.co.uk/news/world/rss.xml',
                'limit': 2,
                'language': 'en'
            },
            {
                'name': 'CNN World',
                'url': 'http://rss.cnn.com/rss/edition.rss',
                'limit': 2,
                'language': 'en'
            }
        ]
        
        all_news = []
        failed_sources = []
        
        logger.info("ğŸš€ Ø´Ø±ÙˆØ¹ ØªØ³Øª ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ...")
        logger.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ù†Ø§Ø¨Ø¹: {len(news_sources)}")
        
        for i, source in enumerate(news_sources, 1):
            logger.info(f"\nğŸ” Ù…Ù†Ø¨Ø¹ {i}/{len(news_sources)}: {source['name']}")
            
            news_items = await self.test_single_rss_feed(
                source['name'], 
                source['url'], 
                source['limit']
            )
            
            if news_items:
                all_news.extend(news_items)
                logger.info(f"âœ… Ù…ÙˆÙÙ‚ÛŒØª: {len(news_items)} Ø®Ø¨Ø±")
            else:
                failed_sources.append(source['name'])
                logger.error(f"âŒ Ø´Ú©Ø³Øª: Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯")
        
        # Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬ ØªØ³Øª:")
        logger.info(f"âœ… Ù…Ù†Ø§Ø¨Ø¹ Ù…ÙˆÙÙ‚: {len(news_sources) - len(failed_sources)}")
        logger.info(f"âŒ Ù…Ù†Ø§Ø¨Ø¹ Ù†Ø§Ù…ÙˆÙÙ‚: {len(failed_sources)}")
        logger.info(f"ğŸ“° Ù…Ø¬Ù…ÙˆØ¹ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø±ÛŒØ§ÙØªÛŒ: {len(all_news)}")
        
        if failed_sources:
            logger.info("âŒ Ù…Ù†Ø§Ø¨Ø¹ Ù†Ø§Ù…ÙˆÙÙ‚:")
            for failed in failed_sources:
                logger.info(f"  - {failed}")
        
        if all_news:
            logger.info("\nğŸ“° Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø±ÛŒØ§ÙØªÛŒ:")
            for i, news in enumerate(all_news[:3], 1):
                logger.info(f"  {i}. {news['source']}: {news['title'][:60]}...")
        
        return all_news, failed_sources
    
    async def test_specific_issue(self):
        """ØªØ³Øª Ù…Ø´Ú©Ù„ Ø®Ø§Øµ Ú¯Ø²Ø§Ø±Ø´ Ø´Ø¯Ù‡"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ ØªØ³Øª Ù…Ø´Ú©Ù„ Ø®Ø§Øµ Ú¯Ø²Ø§Ø±Ø´ Ø´Ø¯Ù‡...")
        logger.info("Ù…Ø´Ú©Ù„: 'ÙÙ‚Ø· 5 ØªØ§ Ø®Ø¨Ø± Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒÚ©Ù†Ù… Ø§ÙˆÙ†Ù… Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ú©Ù‡ 10 ØªØ§ Ø¨Ø´Ù‡'")
        logger.info("="*80)
        
        all_news, failed_sources = await self.test_all_news_sources()
        
        logger.info(f"\nğŸ¯ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ:")
        logger.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø±ÛŒØ§ÙØªÛŒ: {len(all_news)}")
        logger.info(f"ğŸ¯ Ù‡Ø¯Ù: 10+ Ø§Ø®Ø¨Ø§Ø±")
        
        if len(all_news) < 10:
            logger.error(f"âŒ Ù…Ø´Ú©Ù„ ØªØ£ÛŒÛŒØ¯ Ø´Ø¯: ÙÙ‚Ø· {len(all_news)} Ø®Ø¨Ø± Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯Ù‡")
            logger.error("ğŸ” Ø¹Ù„Ù„ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ:")
            logger.error("  1. Ø¨Ø±Ø®ÛŒ Ù…Ù†Ø§Ø¨Ø¹ RSS Ú©Ø§Ø± Ù†Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯")
            logger.error("  2. Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ø®Ø¨Ø± Ø¯Ø± Ù‡Ø± Ù…Ù†Ø¨Ø¹ Ú©Ù… Ø§Ø³Øª")
            logger.error("  3. Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ XML")
        else:
            logger.info(f"âœ… Ù…Ø´Ú©Ù„ Ø­Ù„ Ø´Ø¯Ù‡: {len(all_news)} Ø®Ø¨Ø± Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯Ù‡")
        
        return len(all_news), failed_sources

async def main():
    """ØªØ³Øª Ø§ØµÙ„ÛŒ"""
    logger.info("ğŸ”§ Ø´Ø±ÙˆØ¹ Ø¹ÛŒØ¨â€ŒÛŒØ§Ø¨ÛŒ Ø¯Ù‚ÛŒÙ‚ Ø§Ø®Ø¨Ø§Ø± Ø¹Ù…ÙˆÙ…ÛŒ...")
    logger.info(f"ğŸ• Ø²Ù…Ø§Ù† Ø´Ø±ÙˆØ¹: {datetime.now()}")
    
    async with DetailedNewsFetcher() as fetcher:
        try:
            news_count, failed_sources = await fetcher.test_specific_issue()
            
            logger.info(f"\nğŸ Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ:")
            logger.info(f"ğŸ“° ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±: {news_count}")
            logger.info(f"âŒ Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø±Ø§Ø¨: {len(failed_sources)}")
            
            if failed_sources:
                logger.info("\nğŸ”§ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø±Ø§ÛŒ Ø­Ù„ Ù…Ø´Ú©Ù„:")
                for failed in failed_sources:
                    logger.info(f"  - Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù†Ø¨Ø¹: {failed}")
            
            logger.info(f"\nğŸ• Ø²Ù…Ø§Ù† Ù¾Ø§ÛŒØ§Ù†: {datetime.now()}")
            
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± ØªØ³Øª: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())