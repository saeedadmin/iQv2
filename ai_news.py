#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Ù…Ø§Ú˜ÙˆÙ„ Ø§Ø®Ø¨Ø§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± AI Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø¹ØªØ¨Ø±
Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡: MiniMax Agent
"""

import asyncio
import aiohttp
import feedparser
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

# Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø¨Ø±ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
AI_NEWS_SOURCES = [
    {
        'name': 'TechCrunch AI',
        'url': 'https://techcrunch.com/category/artificial-intelligence/feed/',
        'icon': 'ğŸš€'
    },
    {
        'name': 'VentureBeat AI',
        'url': 'https://venturebeat.com/category/ai/feed/',
        'icon': 'ğŸ“¡'
    },
    {
        'name': 'The Verge AI',
        'url': 'https://www.theverge.com/rss/ai-artificial-intelligence/index.xml',
        'icon': 'ğŸ”¬'
    },
    {
        'name': 'AI News',
        'url': 'https://www.artificialintelligence-news.com/feed/',
        'icon': 'ğŸ¤–'
    }
]

async def fetch_rss_feed(session, source):
    """Ø¯Ø±ÛŒØ§ÙØª ÛŒÚ© RSS feed"""
    try:
        async with session.get(source['url'], timeout=10) as response:
            if response.status == 200:
                content = await response.text()
                feed = feedparser.parse(content)
                
                entries = []
                for entry in feed.entries[:3]:  # ÙÙ‚Ø· 3 Ø®Ø¨Ø± Ø§Ø² Ù‡Ø± Ù…Ù†Ø¨Ø¹
                    # ØªØ¨Ø¯ÛŒÙ„ ØªØ§Ø±ÛŒØ®
                    pub_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        pub_date = datetime(*entry.updated_parsed[:6])
                    else:
                        pub_date = datetime.now()
                    
                    # Ø®Ù„Ø§ØµÙ‡ Ù…ØªÙ†
                    summary = ""
                    if hasattr(entry, 'summary'):
                        summary = entry.summary[:150] + "..." if len(entry.summary) > 150 else entry.summary
                    elif hasattr(entry, 'description'):
                        summary = entry.description[:150] + "..." if len(entry.description) > 150 else entry.description
                    
                    entries.append({
                        'title': entry.title,
                        'link': entry.link,
                        'summary': summary,
                        'pub_date': pub_date,
                        'source': source['name'],
                        'icon': source['icon']
                    })
                
                return entries
            else:
                logger.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª {source['name']}: HTTP {response.status}")
                return []
                
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² {source['name']}: {e}")
        return []

async def get_ai_news():
    """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ"""
    try:
        all_entries = []
        
        # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹
        async with aiohttp.ClientSession() as session:
            tasks = [fetch_rss_feed(session, source) for source in AI_NEWS_SOURCES]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, list):
                    all_entries.extend(result)
        
        if not all_entries:
            return "âŒ Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ø§Ù…Ú©Ø§Ù† Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯."
        
        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ® Ø§Ù†ØªØ´Ø§Ø±
        all_entries.sort(key=lambda x: x['pub_date'], reverse=True)
        
        # Ø§Ù†ØªØ®Ø§Ø¨ 8 Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ†
        latest_entries = all_entries[:8]
        
        # ÙØ±Ù…Øªâ€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ§Ù…
        formatted_news = "ğŸ¤– **Ø¢Ø®Ø±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ**\n\n"
        
        for i, entry in enumerate(latest_entries, 1):
            # Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ HTML Ø§Ø² Ø¹Ù†ÙˆØ§Ù† Ùˆ Ø®Ù„Ø§ØµÙ‡
            title = entry['title'].replace('<', '&lt;').replace('>', '&gt;')
            summary = entry['summary'].replace('<', '&lt;').replace('>', '&gt;')
            
            formatted_news += f"{entry['icon']} **[{title}]({entry['link']})**\n"
            formatted_news += f"ğŸ“¡ Ù…Ù†Ø¨Ø¹: {entry['source']}\n"
            
            if summary:
                formatted_news += f"ğŸ“ {summary}\n"
            
            # ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† Ø§Ø®Ø¨Ø§Ø±
            if i < len(latest_entries):
                formatted_news += "\n"
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† footer
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
        formatted_news += f"\nâ° Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {current_time}\n"
        formatted_news += f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø¨Ø¹: {len(AI_NEWS_SOURCES)} | ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±: {len(latest_entries)}"
        
        return formatted_news
        
    except Exception as e:
        logger.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ: {e}")
        return f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø±: {str(e)}"

# ØªØ³Øª ÙØ§Ù†Ú©Ø´Ù†
async def test_ai_news():
    """ØªØ³Øª Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø±"""
    print("Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ...")
    news = await get_ai_news()
    print(news)

if __name__ == "__main__":
    asyncio.run(test_ai_news())
