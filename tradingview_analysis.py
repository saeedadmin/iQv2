#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
TradingView Analysis Fetcher - Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ TradingView Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
"""

import aiohttp
import asyncio
import re
import html
import datetime
from typing import Dict, Any, Optional, List
from bs4 import BeautifulSoup

class TradingViewAnalysisFetcher:
    def __init__(self):
        self.base_url = "https://www.tradingview.com"
        self.community_url = "https://www.tradingview.com/ideas/search/"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def validate_crypto_pair_format(self, pair: str) -> bool:
        """Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ ÙØ±Ù…Øª Ø¬ÙØª Ø§Ø±Ø² - ÙÙ‚Ø· ÙØ±Ù…Øª Ù…Ø§Ù†Ù†Ø¯ btcusdt Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ Ø§Ø³Øª"""
        # ÙÙ‚Ø· Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©ØŒ Ø¨Ø¯ÙˆÙ† ÙØ§ØµÙ„Ù‡ØŒ Ø¨Ø¯ÙˆÙ† Ù†Ø´Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ
        pattern = r'^[a-z]+usdt$'
        return bool(re.match(pattern, pair))
    
    def extract_symbol_from_pair(self, pair: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³ÛŒÙ…Ø¨Ù„ Ø§ØµÙ„ÛŒ Ø§Ø² Ø¬ÙØª Ø§Ø±Ø²"""
        if pair.endswith('usdt'):
            return pair[:-4]  # Ø­Ø°Ù 'usdt' Ø§Ø² Ø§Ù†ØªÙ‡Ø§
        return pair
    
    async def scrape_community_analysis(self, symbol: str) -> Optional[Dict[str, Any]]:
        """Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² Ú©Ø§Ù…ÛŒÙˆÙ†ÛŒØªÛŒ TradingView"""
        try:
            timeout = aiohttp.ClientTimeout(total=15)
            async with aiohttp.ClientSession(timeout=timeout, headers=self.headers) as session:
                # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¨Ø®Ø´ Ideas Ø¨Ø±Ø§ÛŒ Ø³ÛŒÙ…Ø¨Ù„ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
                search_url = f"https://www.tradingview.com/symbols/{symbol}/ideas/"
                
                async with session.get(search_url) as response:
                    if response.status == 200:
                        content = await response.text()
                        return self.parse_community_content(content, symbol)
                    else:
                        return None
                        
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± scraping: {e}")
            return None
    
    def parse_community_content(self, content: str, symbol: str) -> Optional[Dict[str, Any]]:
        """Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù…ÛŒÙˆÙ†ÛŒØªÛŒ TradingView"""
        try:
            soup = BeautifulSoup(content, 'html.parser')
            
            # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù‡Ù…Ù‡ Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·
            candidate_ideas = []
            idea_links = soup.find_all('a', href=True)
            
            for link in idea_links:
                href = link.get('href', '')
                # Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø±Ø§ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¯Ù‡ Ø§ØµÙ„ÛŒ TradingView
                # Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø´Ø§Ù…Ù„ /chart/ Ùˆ Ø´Ù†Ø§Ø³Ù‡ ØªØ­Ù„ÛŒÙ„ Ù‡Ø³ØªÙ†Ø¯
                if ('/chart/' in href and 
                    '/' in href.split('/chart/')[-1] and 
                    any(char.isalnum() for char in href) and 
                    len(href.split('/chart/')[-1]) > 10 and
                    '-' in href.split('/chart/')[-1]):
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ title Ø§Ø² Ù…ØªÙ† Ù„ÛŒÙ†Ú© ÛŒØ§ Ø§Ø² Ø¹Ù†ØµØ± ÙˆØ§Ù„Ø¯
                    title = link.get_text(strip=True) or link.get('title', '')
                    
                    # Ø§Ú¯Ø± title Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø¨ÙˆØ¯ØŒ Ø§Ø² Ø¹Ù†ØµØ± ÙˆØ§Ù„Ø¯ Ø¨Ú¯ÛŒØ±
                    if not title or len(title) < 5:
                        parent = link.parent
                        if parent:
                            title = parent.get_text(strip=True)[:100]
                    
                    if title and len(title) > 5:
                        # ØªØ´Ú©ÛŒÙ„ URL Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¯Ù‡
                        analysis_url = href if href.startswith('http') else f"https://www.tradingview.com{href}"
                        
                        # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø¹Ú©Ø³ Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ù„ÛŒÙ†Ú©
                        image_url = self.find_related_image(soup, link)
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
                        description = self.extract_description_from_soup(soup, link)
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡
                        author = self.extract_author_from_soup(soup, link)
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø± (Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
                        publish_time = self.extract_publish_time(soup, link)
                        
                        idea_data = {
                            'title': title,
                            'description': description,
                            'analysis_url': analysis_url,
                            'image_url': image_url,
                            'author': author,
                            'publish_time': publish_time,
                            'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M')
                        }
                        
                        candidate_ideas.append(idea_data)
            
            # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ø§ÛŒØ¯Ù‡â€ŒØ§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯
            if not candidate_ideas:
                return None
            
            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø± (Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ø§ÙˆÙ„)
            # Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§Ù„Ø§ØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯
            candidate_ideas.sort(key=lambda x: (
                x['publish_time'] is not None,  # Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø²Ù…Ø§Ù† Ø§ÙˆÙ„
                x['publish_time'] if x['publish_time'] else datetime.datetime.min,
                len(x['description'])  # ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ù„Ù†Ø¯ØªØ± Ø§ÙˆÙ„ÙˆÛŒØª Ø¨ÛŒØ´ØªØ±
            ), reverse=True)
            
            # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ùˆ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§ÛŒØ¯Ù‡
            return candidate_ideas[0]
            
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± parse_community_content: {e}")
            return None
    
    def find_related_image(self, soup: BeautifulSoup, link_element) -> Optional[str]:
        """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¹Ú©Ø³ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù„ÛŒÙ†Ú© ØªØ­Ù„ÛŒÙ„"""
        try:
            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ù„ÛŒÙ†Ú©
            parent = link_element.parent
            if parent:
                imgs = parent.find_all('img', src=True)
                for img in imgs:
                    src = img['src']
                    if 'tradingview.com' in src and '_mid.png' in src:
                        return src
            
            # Ø¬Ø³ØªØ¬Ùˆ Ú©Ù„ÛŒ Ø¯Ø± ØµÙØ­Ù‡
            all_imgs = soup.find_all('img', src=True)
            for img in all_imgs:
                src = img['src']
                if 'tradingview.com' in src and '_mid.png' in src:
                    return src
            
            return None
        except:
            return None
    
    def extract_description_from_soup(self, soup: BeautifulSoup, link_element) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ Ø§Ø² soup"""
        try:
            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¹Ù†Ø§ØµØ± Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙˆØ¶ÛŒØ­Ø§Øª
            parent = link_element.parent
            description_candidates = []
            
            if parent:
                # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¹Ù†Ø§ØµØ± Ù…Ø¬Ø§ÙˆØ±
                for sibling in parent.find_next_siblings()[:5]:
                    text = sibling.get_text(strip=True)
                    if len(text) > 30 and len(text) < 500:
                        description_candidates.append(text)
                
                # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¹Ù†Ø§ØµØ± ÙØ±Ø²Ù†Ø¯
                for child in parent.find_all(['p', 'div', 'span'])[:10]:
                    text = child.get_text(strip=True)
                    if len(text) > 30 and len(text) < 500:
                        description_candidates.append(text)
            
            # Ø¬Ø³ØªØ¬Ùˆ Ú©Ù„ÛŒ Ø¯Ø± ØµÙØ­Ù‡ Ø¨Ø±Ø§ÛŒ div Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø´Ø§Ù…Ù„ ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨Ø§Ø´Ù†Ø¯
            for div in soup.find_all('div', class_=True)[:20]:
                class_names = ' '.join(div.get('class', []))
                if any(keyword in class_names.lower() for keyword in ['content', 'description', 'text', 'body']):
                    text = div.get_text(strip=True)
                    if len(text) > 50 and len(text) < 800:
                        description_candidates.append(text)
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† ØªÙˆØ¶ÛŒØ­Ø§Øª
            if description_candidates:
                # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„ (ØªØ±Ø¬ÛŒØ­ Ù…ØªÙ† Ø¨Ø§ Ø·ÙˆÙ„ Ù…ØªÙˆØ³Ø·)
                description_candidates.sort(key=lambda x: abs(len(x) - 200))
                best_desc = description_candidates[0]
                return best_desc[:400] + "..." if len(best_desc) > 400 else best_desc
            
            return "ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ù…ÛŒÙˆÙ†ÛŒØªÛŒ TradingView - Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ± Ø¨Ø§ Ú©Ù„ÛŒÚ© Ø±ÙˆÛŒ Ù„ÛŒÙ†Ú©"
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± extract_description: {e}")
            return "ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ù…ÛŒÙˆÙ†ÛŒØªÛŒ TradingView"
    
    def extract_author_from_soup(self, soup: BeautifulSoup, link_element) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ ØªØ­Ù„ÛŒÙ„"""
        try:
            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù†Ø²Ø¯ÛŒÚ©ÛŒ Ù„ÛŒÙ†Ú© Ø¨Ø±Ø§ÛŒ Ù†Ø§Ù… Ú©Ø§Ø±Ø¨Ø±ÛŒ
            parent = link_element.parent
            if parent:
                # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø¹Ù†Ø§ØµØ±ÛŒ Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø´Ø§Ù…Ù„ Ù†Ø§Ù… Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø§Ø´Ù†Ø¯
                for element in parent.find_all(['span', 'div', 'a'])[:10]:
                    text = element.get_text(strip=True)
                    class_names = ' '.join(element.get('class', []))
                    
                    # Ø§Ú¯Ø± Ø´Ø§Ù…Ù„ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§Ø´Ø¯
                    if (any(keyword in class_names.lower() for keyword in ['user', 'author', 'name']) or
                        (len(text) > 2 and len(text) < 30 and '@' not in text and 
                         not any(char.isdigit() for char in text) and text.count(' ') <= 2)):
                        return text
            
            # Ø¬Ø³ØªØ¬Ùˆ Ú©Ù„ÛŒ Ø¯Ø± ØµÙØ­Ù‡
            for element in soup.find_all(['span', 'div', 'a'], class_=True)[:30]:
                class_names = ' '.join(element.get('class', []))
                if any(keyword in class_names.lower() for keyword in ['user', 'author', 'username']):
                    text = element.get_text(strip=True)
                    if len(text) > 2 and len(text) < 30:
                        return text
            
            return 'TradingView Community'
        except:
            return 'TradingView Community'
    
    def extract_publish_time(self, soup: BeautifulSoup, link_element) -> Optional[datetime.datetime]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ´Ø§Ø± ØªØ­Ù„ÛŒÙ„"""
        try:
            parent = link_element.parent
            if parent:
                # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø¹Ù†Ø§ØµØ±ÛŒ Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø´Ø§Ù…Ù„ Ø²Ù…Ø§Ù† Ø¨Ø§Ø´Ù†Ø¯
                time_elements = parent.find_all(['time', 'span'])
                for element in time_elements:
                    # Ø¨Ø±Ø±Ø³ÛŒ datetime attribute
                    if element.get('datetime'):
                        try:
                            return datetime.datetime.fromisoformat(element['datetime'].replace('Z', '+00:00'))
                        except:
                            pass
                    
                    # Ø¨Ø±Ø±Ø³ÛŒ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
                    text = element.get_text(strip=True)
                    time_patterns = [
                        r'(\d+)\s*hour[s]?\s*ago',
                        r'(\d+)\s*day[s]?\s*ago',
                        r'(\d+)\s*week[s]?\s*ago',
                        r'(\d+)\s*month[s]?\s*ago'
                    ]
                    
                    for pattern in time_patterns:
                        match = re.search(pattern, text, re.IGNORECASE)
                        if match:
                            number = int(match.group(1))
                            if 'hour' in text.lower():
                                return datetime.datetime.now() - datetime.timedelta(hours=number)
                            elif 'day' in text.lower():
                                return datetime.datetime.now() - datetime.timedelta(days=number)
                            elif 'week' in text.lower():
                                return datetime.datetime.now() - datetime.timedelta(weeks=number)
                            elif 'month' in text.lower():
                                return datetime.datetime.now() - datetime.timedelta(days=number*30)
            
            return None
        except:
            return None
    
    def normalize_to_usdt_pair_DEPRECATED(self, crypto_input: str) -> Optional[str]:
        """ØªØ¨Ø¯ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø± Ø¨Ù‡ ÙØ±Ù…Øª Ø¬ÙØª Ø§Ø±Ø² USDT"""
        if not crypto_input:
            return None
            
        # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ lowercase
        clean_input = crypto_input.strip().lower().replace(' ', '')
        
        # Ø§Ú¯Ø± ÙˆØ±ÙˆØ¯ÛŒ Ø´Ø§Ù…Ù„ usdt Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†
        if 'usdt' in clean_input:
            # Ù…Ø«Ø§Ù„: ethusdt -> ETHUSDT
            if clean_input.endswith('usdt'):
                symbol = clean_input[:-4].upper()
                return f"{symbol}USDT"
            # Ù…Ø«Ø§Ù„: eth/usdt -> ETHUSDT  
            elif '/usdt' in clean_input:
                symbol = clean_input.replace('/usdt', '').upper()
                return f"{symbol}USDT"
                
        # Ù†Ù‚Ø´Ù‡ Ø§Ø±Ø²Ù‡Ø§ Ø¨Ù‡ USDT pairs
        crypto_map = {
            'bitcoin': 'BTCUSDT',
            'btc': 'BTCUSDT', 
            'Ø¨ÛŒØª Ú©ÙˆÛŒÙ†': 'BTCUSDT',
            'Ø¨ÛŒØªÚ©ÙˆÛŒÙ†': 'BTCUSDT',
            'ethereum': 'ETHUSDT',
            'eth': 'ETHUSDT',
            'Ø§ØªØ±ÛŒÙˆÙ…': 'ETHUSDT',
            'Ø§ØªØ±': 'ETHUSDT',
            'solana': 'SOLUSDT',
            'sol': 'SOLUSDT',
            'Ø³ÙˆÙ„Ø§Ù†Ø§': 'SOLUSDT',
            'cardano': 'ADAUSDT',
            'ada': 'ADAUSDT',
            'Ú©Ø§Ø±Ø¯Ø§Ù†Ùˆ': 'ADAUSDT',
            'binance coin': 'BNBUSDT',
            'bnb': 'BNBUSDT',
            'Ø¨Ø§ÛŒÙ†Ù†Ø³': 'BNBUSDT',
            'xrp': 'XRPUSDT',
            'ripple': 'XRPUSDT',
            'Ø±ÛŒÙ¾Ù„': 'XRPUSDT',
            'dogecoin': 'DOGEUSDT',
            'doge': 'DOGEUSDT',
            'Ø¯ÙˆØ¬': 'DOGEUSDT',
            'Ø¯ÙˆØ¬ Ú©ÙˆÛŒÙ†': 'DOGEUSDT',
            'chainlink': 'LINKUSDT',
            'link': 'LINKUSDT',
            'Ú†ÛŒÙ† Ù„ÛŒÙ†Ú©': 'LINKUSDT',
            'litecoin': 'LTCUSDT',
            'ltc': 'LTCUSDT',
            'Ù„Ø§ÛŒØª Ú©ÙˆÛŒÙ†': 'LTCUSDT',
            'polkadot': 'DOTUSDT',
            'dot': 'DOTUSDT',
            'Ù¾ÙˆÙ„Ú©Ø§Ø¯Ø§Øª': 'DOTUSDT',
            'avalanche': 'AVAXUSDT',
            'avax': 'AVAXUSDT',
            'Ø§ÙˆÙ„Ø§Ù†Ú†': 'AVAXUSDT',
            # Ø§Ø±Ø²Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
            'matic': 'MATICUSDT',
            'polygon': 'MATICUSDT',
            'uni': 'UNIUSDT',
            'uniswap': 'UNIUSDT',
            'atom': 'ATOMUSDT',
            'cosmos': 'ATOMUSDT'
        }
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù‚Ø´Ù‡ Ø§Ø±Ø²Ù‡Ø§
        if clean_input in crypto_map:
            return crypto_map[clean_input]
            
        # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ±Ù…Øª
        # Ù…Ø«Ø§Ù„: eth -> ETHUSDT
        if len(clean_input) <= 10 and clean_input.isalpha():
            potential_symbol = f"{clean_input.upper()}USDT"
            return potential_symbol
            
        return None
    
    async def scrape_live_analysis(self, symbol: str) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ Ø²Ù†Ø¯Ù‡ Ø§Ø² TradingView"""
        try:
            # URL Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ÛŒ TradingView
            url = f"{self.base_url}/symbols/{symbol}/ideas/"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=self.headers, timeout=10) as response:
                    if response.status == 200:
                        content = await response.text()
                        
                        # Ù¾Ø§Ø±Ø³ Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§ Ø¨Ø±Ø§ÛŒ ÛŒØ§ÙØªÙ† Ø¢Ø®Ø±ÛŒÙ† ØªØ­Ù„ÛŒÙ„
                        soup = BeautifulSoup(content, 'html.parser')
                        
                        # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ - Ú†Ù†Ø¯ÛŒÙ† selector Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†
                        idea_selectors = [
                            'div[data-testid="idea-item"]',
                            '.tv-idea-card',
                            '.idea-card',
                            'article[data-testid="idea-card"]'
                        ]
                        
                        idea_elements = []
                        for selector in idea_selectors:
                            idea_elements = soup.select(selector)
                            if idea_elements:
                                break
                        
                        if idea_elements:
                            # Ú¯Ø±ÙØªÙ† Ø§ÙˆÙ„ÛŒÙ† (Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ†) Ø§ÛŒØ¯Ù‡
                            first_idea = idea_elements[0]
                            
                            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
                            title_elem = first_idea.find('h3') or first_idea.find('a')
                            title = title_elem.get_text(strip=True) if title_elem else f"ØªØ­Ù„ÛŒÙ„ {symbol}"
                            
                            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©
                            link_elem = first_idea.find('a', href=True)
                            analysis_url = f"{self.base_url}{link_elem['href']}" if link_elem else f"{self.base_url}/symbols/{symbol}/"
                            
                            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØµÙˆÛŒØ±
                            img_elem = first_idea.find('img')
                            image_url = img_elem.get('src') or img_elem.get('data-src') if img_elem else None
                            
                            return {
                                'success': True,
                                'title': title,
                                'analysis_url': analysis_url,
                                'image_url': image_url,
                                'symbol': symbol,
                                'description': f"ğŸ”¥ Ø¢Ø®Ø±ÛŒÙ† ØªØ­Ù„ÛŒÙ„ {symbol} Ø§Ø² TradingView\n\nğŸ“Š {title}\n\nâ° Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}",
                                'timestamp': datetime.datetime.now().isoformat()
                            }
                        
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ Ø²Ù†Ø¯Ù‡: {e}")
            
        return {'success': False}
    
    def get_fallback_analysis(self, crypto_pair: str) -> Dict[str, Any]:
        """Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ fallback Ø¨Ø±Ø§ÛŒ Ø²Ù…Ø§Ù† Ø¹Ø¯Ù… Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ TradingView"""
        fallback_data = {
            'btcusdt': {
                'title': 'Bitcoin Technical Analysis - Community Insights',
                'description': 'ğŸ“Š ØªØ­Ù„ÛŒÙ„ ÙÙ†ÛŒ Ø¨ÛŒØª Ú©ÙˆÛŒÙ†: Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø·ÙˆØ­ Ø­Ù…Ø§ÛŒØª Ùˆ Ù…Ù‚Ø§ÙˆÙ…Øª Ú©Ù„ÛŒØ¯ÛŒØŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú†Ø§Ø±Øª Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø­Ø±Ú©Øª Ù‚ÛŒÙ…Øª Ø¯Ø± Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÛŒØ§Ù† Ù…Ø¯Øª. ØªØ­Ù„ÛŒÙ„ Ø­Ø¬Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ùˆ momentum indicators.',
                'analysis_url': f'https://www.tradingview.com/symbols/{crypto_pair.upper()}/ideas/',
                'image_url': 'https://s3.tradingview.com/5/5HqYVVyh_mid.png',
                'author': 'TradingView Community',
                'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M')
            },
            'ethusdt': {
                'title': 'Ethereum Price Action Analysis',
                'description': 'ğŸ”® ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‚ÛŒÙ…Øª Ø§ØªØ±ÛŒÙˆÙ…: Ø¨Ø±Ø±Ø³ÛŒ Ø±ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±ØŒ Ø³Ø·ÙˆØ­ ÙÛŒØ¨ÙˆÙ†Ø§Ú†ÛŒØŒ Ùˆ Ø§Ø­ØªÙ…Ø§Ù„ Ø´Ú©Ø³Øª Ø§Ø² Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù‚ÛŒÙ…ØªÛŒ. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ÙØ§Ú©ØªÙˆØ±Ù‡Ø§ÛŒ ØªÚ©Ù†ÛŒÚ©Ø§Ù„ Ùˆ Ø¨Ù†ÛŒØ§Ø¯ÛŒ.',
                'analysis_url': f'https://www.tradingview.com/symbols/{crypto_pair.upper()}/ideas/',
                'image_url': 'https://s3.tradingview.com/k/kVfkJOXh_mid.png',
                'author': 'TradingView Community', 
                'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M')
            },
            'solusdt': {
                'title': 'Solana Market Outlook & Strategy',
                'description': 'âš¡ Ù†Ú¯Ø±Ø´ Ø¨Ø§Ø²Ø§Ø± Ø³ÙˆÙ„Ø§Ù†Ø§: Ø¨Ø±Ø±Ø³ÛŒ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø±Ø´Ø¯ØŒ ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±ÛŒ Ùˆ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù‚Ø¯Ø±Øª Ø®Ø±ÛŒØ¯Ø§Ø±Ø§Ù† vs ÙØ±ÙˆØ´Ù†Ø¯Ú¯Ø§Ù†.',
                'analysis_url': f'https://www.tradingview.com/symbols/{crypto_pair.upper()}/ideas/',
                'image_url': 'https://s3.tradingview.com/3/3jFcSQDp_mid.png',
                'author': 'TradingView Community',
                'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M')
            }
        }
        
        data = fallback_data.get(crypto_pair.lower())
        if data:
            return {
                'success': True,
                'crypto': self.extract_symbol_from_pair(crypto_pair),
                'title': data['title'],
                'description': data['description'],
                'analysis_url': data['analysis_url'],
                'image_url': data['image_url'],
                'symbol': crypto_pair.upper(),
                'author': data['author'],
                'timestamp': data['timestamp'],
                'source': 'TradingView Community (Cached)'
            }
        return None
    
    async def fetch_latest_analysis(self, crypto_pair: str) -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…ÛŒÙˆÙ†ÛŒØªÛŒ Ø¨Ø±Ø§ÛŒ Ø¬ÙØª Ø§Ø±Ø² Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡"""
        try:
            # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ ÙØ±Ù…Øª ÙˆØ±ÙˆØ¯ÛŒ
            if not self.validate_crypto_pair_format(crypto_pair):
                return {
                    'success': False,
                    'error': f"âŒ ÙØ±Ù…Øª Ù†Ø§Ø¯Ø±Ø³Øª!\n\nâœ… ÙØ±Ù…Øª ØµØ­ÛŒØ­: Ù…Ø«Ù„ `btcusdt`\n\nğŸ“ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø±:\nâ€¢ btcusdt\nâ€¢ ethusdt\nâ€¢ solusdt\nâ€¢ adausdt\nâ€¢ bnbusdt\nâ€¢ xrpusdt\nâ€¢ dogeusdt\n\nâš ï¸ ÙÙ‚Ø· Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©ØŒ Ø¨Ø¯ÙˆÙ† ÙØ§ØµÙ„Ù‡ ÛŒØ§ Ù†Ø´Ø§Ù†Ù‡"
                }
            
            # Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² Ú©Ø§Ù…ÛŒÙˆÙ†ÛŒØªÛŒ TradingView
            symbol = self.extract_symbol_from_pair(crypto_pair)
            analysis_data = await self.scrape_community_analysis(crypto_pair.upper())
            
            if analysis_data:
                return {
                    'success': True,
                    'crypto': symbol,
                    'title': analysis_data['title'],
                    'description': analysis_data['description'],
                    'analysis_url': analysis_data['analysis_url'],
                    'image_url': analysis_data['image_url'],
                    'symbol': crypto_pair.upper(),
                    'author': analysis_data.get('author', 'TradingView User'),
                    'timestamp': analysis_data.get('timestamp', datetime.datetime.now().strftime('%Y-%m-%d %H:%M')),
                    'source': 'TradingView Community'
                }
            else:
                # fallback Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… Ø¯Ø³ØªØ±Ø³ÛŒ
                fallback_data = self.get_fallback_analysis(crypto_pair)
                return fallback_data
                
        except Exception as e:
            # fallback Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§
            fallback_data = self.get_fallback_analysis(crypto_pair)
            if fallback_data:
                return fallback_data
            return {
                'success': False,
                'error': f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„: {str(e)}\n\nÙ„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯."
            }
    
    def format_analysis_message(self, analysis_data: Dict[str, Any]) -> str:
        """ÙØ±Ù…Øª Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… ØªØ­Ù„ÛŒÙ„ Ø¨Ø±Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…"""
        if analysis_data.get('error'):
            return f"âŒ {analysis_data['error']}"
        
        if not analysis_data.get('success'):
            return f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ {analysis_data.get('crypto', '')}"
        
        crypto_emojis = {
            'btc': 'â‚¿',
            'eth': 'ğŸ”·', 
            'sol': 'âš¡',
            'ada': 'â‚³',
            'bnb': 'ğŸŸ¡',
            'xrp': 'ğŸ”·',
            'doge': 'ğŸ•',
            'link': 'ğŸ”—',
            'ltc': 'Å',
            'dot': 'â—',
            'avax': 'ğŸ”º'
        }
        
        crypto_emoji = crypto_emojis.get(analysis_data['crypto'].lower(), 'ğŸ’°')
        author = analysis_data.get('author', 'TradingView User')
        timestamp = analysis_data.get('timestamp', 'Unknown')
        
        message = f"""
ğŸ“Š *ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…ÛŒÙˆÙ†ÛŒØªÛŒ TradingView*

{crypto_emoji} *Ø¬ÙØª Ø§Ø±Ø²:* {analysis_data.get('symbol', 'N/A')}

ğŸ“ *Ø¹Ù†ÙˆØ§Ù† ØªØ­Ù„ÛŒÙ„:*
{analysis_data['title']}

ğŸ“„ *ØªÙˆØ¶ÛŒØ­Ø§Øª:*
{analysis_data['description']}

ğŸ‘¤ *Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡:* {author}
ğŸ•°ï¸ *Ø²Ù…Ø§Ù†:* {timestamp}

ğŸ”— *Ù„ÛŒÙ†Ú© Ú©Ø§Ù…Ù„:*
[ğŸ‘‰ Ù…Ø´Ø§Ù‡Ø¯Ù‡ ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„]({analysis_data['analysis_url']})

ğŸŒ *Ù…Ù†Ø¨Ø¹:* {analysis_data.get('source', 'TradingView')}
        """
        
        return message.strip()
