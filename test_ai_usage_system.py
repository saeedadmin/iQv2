#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ØªØ³Øª Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… AI usage tracking
Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªÙ…Ø§Ù… Ø§Ø¬Ø²Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ù…ØµØ±Ù AI Ø±Ø§ ØªØ³Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯
"""

import os
import psycopg2
from psycopg2.extras import RealDictCursor
from urllib.parse import urlparse

def test_complete_ai_usage_system():
    """ØªØ³Øª Ú©Ø§Ù…Ù„ Ø³ÛŒØ³ØªÙ… AI usage tracking"""
    
    database_url = os.getenv('DATABASE_URL')
    if not database_url:
        print("âŒ DATABASE_URL not found")
        return False
    
    try:
        # Ø§ØªØµØ§Ù„
        parsed = urlparse(database_url)
        connection_params = {
            'host': parsed.hostname,
            'port': parsed.port or 5432,
            'database': parsed.path[1:],
            'user': parsed.username,
            'password': parsed.password,
            'sslmode': 'require'
        }
        
        conn = psycopg2.connect(**connection_params)
        cursor = conn.cursor(cursor_factory=RealDictCursor)
        
        print("ğŸ¤– Testing Complete AI Usage Tracking System")
        print("=" * 60)
        
        # 1. ØªØ³Øª Ø¬Ø¯ÙˆÙ„
        print("\n1ï¸âƒ£ Testing ai_usage_tracking table...")
        cursor.execute("""
            SELECT COUNT(*) as count FROM information_schema.tables 
            WHERE table_name = 'ai_usage_tracking'
        """)
        if cursor.fetchone()['count'] > 0:
            print("âœ… Table 'ai_usage_tracking' exists")
        else:
            print("âŒ Table 'ai_usage_tracking' not found")
            return False
        
        # 2. ØªØ³Øª Ø³Ø§Ø®ØªØ§Ø± Ø¬Ø¯ÙˆÙ„
        print("\n2ï¸âƒ£ Testing table structure...")
        cursor.execute("""
            SELECT column_name FROM information_schema.columns 
            WHERE table_name = 'ai_usage_tracking'
            ORDER BY ordinal_position
        """)
        
        columns = [row['column_name'] for row in cursor.fetchall()]
        expected_columns = [
            'id', 'provider', 'user_id', 'chat_id', 'model',
            'prompt_tokens', 'completion_tokens', 'total_tokens',
            'rate_limit_info', 'cost_estimate', 'created_at', 'date'
        ]
        
        missing_columns = set(expected_columns) - set(columns)
        if missing_columns:
            print(f"âŒ Missing columns: {missing_columns}")
            return False
        else:
            print("âœ… All expected columns exist")
        
        # 3. ØªØ³Øª Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§
        print("\n3ï¸âƒ£ Testing indexes...")
        cursor.execute("""
            SELECT indexname FROM pg_indexes 
            WHERE tablename = 'ai_usage_tracking'
        """)
        
        indexes = [row['indexname'] for row in cursor.fetchall()]
        print(f"âœ… Found {len(indexes)} indexes: {indexes}")
        
        # 4. ØªØ³Øª ÙˆÛŒÙˆÙ‡Ø§
        print("\n4ï¸âƒ£ Testing views...")
        
        # ØªØ³Øª daily_ai_usage view
        cursor.execute("SELECT * FROM daily_ai_usage LIMIT 1")
        daily_result = cursor.fetchall()
        print(f"âœ… View 'daily_ai_usage' working - {len(daily_result)} records")
        
        # ØªØ³Øª provider_ai_stats view
        cursor.execute("SELECT * FROM provider_ai_stats LIMIT 1")
        provider_result = cursor.fetchall()
        print(f"âœ… View 'provider_ai_stats' working - {len(provider_result)} records")
        
        # 5. ØªØ³Øª Ø¯Ø±Ø¬ Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡
        print("\n5ï¸âƒ£ Testing data insertion...")
        
        test_data = [
            ('groq', 123456789, 987654321, 'llama-3.1-70b-versatile', 150, 200, 350, 0.007),
            ('cerebras', 123456789, 987654321, 'llama-3.1-70b', 120, 180, 300, 0.006),
            ('gemini', 123456789, 987654321, 'gemini-1.5-pro', 100, 150, 250, 0.005),
            ('cohere', 123456789, 987654321, 'command-r-plus', 80, 120, 200, 0.004)
        ]
        
        for provider, user_id, chat_id, model, prompt_tokens, completion_tokens, total_tokens, cost in test_data:
            cursor.execute("""
                INSERT INTO ai_usage_tracking 
                (provider, user_id, chat_id, model, prompt_tokens, completion_tokens, total_tokens, cost_estimate)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """, (provider, user_id, chat_id, model, prompt_tokens, completion_tokens, total_tokens, cost))
        
        conn.commit()
        print(f"âœ… Inserted {len(test_data)} test records")
        
        # 6. ØªØ³Øª ÙˆÛŒÙˆÙ‡Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡
        print("\n6ï¸âƒ£ Testing views with data...")
        
        cursor.execute("SELECT * FROM daily_ai_usage ORDER BY date DESC")
        daily_data = cursor.fetchall()
        print(f"ğŸ“Š Daily usage view - {len(daily_data)} rows")
        if daily_data:
            row = daily_data[0]
            print(f"   Provider: {row['provider']}, Date: {row['date']}, Requests: {row['total_requests']}")
        
        cursor.execute("SELECT * FROM provider_ai_stats ORDER BY total_tokens_used DESC")
        provider_data = cursor.fetchall()
        print(f"ğŸ“Š Provider stats view - {len(provider_data)} rows")
        if provider_data:
            row = provider_data[0]
            print(f"   Provider: {row['provider']}, Total tokens: {row['total_tokens_used']}")
        
        # 7. ØªØ³Øª JSONB field
        print("\n7ï¸âƒ£ Testing JSONB rate_limit_info field...")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÛŒÚ© Ø±Ú©ÙˆØ±Ø¯ Ø¨Ø§ rate_limit_info
        cursor.execute("""
            INSERT INTO ai_usage_tracking 
            (provider, user_id, model, total_tokens, rate_limit_info)
            VALUES ('cerebras', 999888777, 'llama-3.1-70b', 100, '{"tokens_remaining_minute": 50000, "tokens_limit_minute": 100000}')
        """)
        conn.commit()
        print("âœ… JSONB rate_limit_info field working")
        
        # 8. ØªØ³Øª Ø¢Ù…Ø§Ø±
        print("\n8ï¸âƒ£ Testing statistics...")
        
        # ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§
        cursor.execute("SELECT COUNT(*) as total FROM ai_usage_tracking")
        total_records = cursor.fetchone()['total']
        print(f"ğŸ“ˆ Total records: {total_records}")
        
        # Ù…Ø¬Ù…ÙˆØ¹ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§
        cursor.execute("SELECT SUM(total_tokens) as total_tokens FROM ai_usage_tracking")
        total_tokens = cursor.fetchone()['total_tokens']
        print(f"ğŸ“ˆ Total tokens used: {total_tokens:,}")
        
        # Ø¨Ø± Ø§Ø³Ø§Ø³ provider
        cursor.execute("""
            SELECT provider, COUNT(*) as count, SUM(total_tokens) as tokens 
            FROM ai_usage_tracking 
            GROUP BY provider
            ORDER BY tokens DESC
        """)
        
        for row in cursor.fetchall():
            print(f"   {row['provider']}: {row['count']} requests, {row['tokens']:,} tokens")
        
        # 9. Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ØªØ³Øª Ø¯Ø§Ø¯Ù‡
        print("\n9ï¸âƒ£ Cleaning up test data...")
        cursor.execute("DELETE FROM ai_usage_tracking WHERE model LIKE 'llama-%' OR model LIKE 'gemini-%' OR model LIKE 'command-%'")
        conn.commit()
        print("âœ… Test data cleaned up")
        
        cursor.close()
        conn.close()
        
        print("\nğŸ‰ All tests passed! AI Usage Tracking System is fully functional!")
        return True
        
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def show_system_summary():
    """Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡ Ø³ÛŒØ³ØªÙ…"""
    print("\nğŸ“‹ AI Usage Tracking System Summary")
    print("=" * 50)
    print("âœ… Database table: ai_usage_tracking")
    print("âœ… Views: daily_ai_usage, provider_ai_stats")  
    print("âœ… Indexes: optimized for performance")
    print("âœ… Features: token tracking, cost estimation, rate limit monitoring")
    print("âœ… Providers: Groq, Cerebras, Gemini, Cohere")
    print("\nğŸ”„ Ready for admin panel testing!")

if __name__ == "__main__":
    print("ğŸš€ Starting AI Usage Tracking System Test")
    
    if test_complete_ai_usage_system():
        show_system_summary()
    else:
        print("\nâŒ System test failed. Please check the errors above.")